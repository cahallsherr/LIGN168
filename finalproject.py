# -*- coding: utf-8 -*-
"""finalproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hN_rqIUOCea-1Fvxk3dcurebYvHir9sf
"""

# install essential libraries
!pip3 install librosa
!pip install pydub
!pip install torch torchvision torchaudio
!pip install datasets transformers
!pip install praatio librosa
!pip install g2p_en g2pM
!pip install sounddevice
!pip install wavio
!pip install scipy
!apt-get update
!apt-get install libsndfile1

import os
from os.path import exists, join, expanduser

os.chdir(expanduser("~"))
charsiu_dir = 'charsiu'
if exists(charsiu_dir):
  !rm -rf /root/charsiu
if not exists(charsiu_dir):
  ! git clone -b development https://github.com/lingjzhu/$charsiu_dir
  ! cd charsiu && git checkout && cd -

os.chdir(charsiu_dir)

# import essential libraries for charsiu
import sys
import torch
from itertools import groupby

sys.path.insert(0,'src')

# [optional step for using with colab:] use your own data from your google drive
from google.colab import drive
drive.mount('/content/drive/')

#initializing a pretrained charsiu model. This one is textless

from Charsiu import charsiu_predictive_aligner

#initialize charsiu model

charsiu = charsiu_predictive_aligner(aligner='charsiu/en_w2v2_fc_10ms')

from pydub import AudioSegment

def give_me_music(wavfile_name):#function to get an instrumental version of speech stream
"""
    Convert a wav file of speech to instrumental sounds.

    First the function resamples the orignal audio to 16000 Hz. This function uses a forced
    aligner to segment speech sounds. Then it loops though speech sounds to create a list
    with the types of speech sounds present. Then the list of types is looped through and
    specific instrumental sounds are added to an empty audio segment. The instrumental audios
    are also cliped to the length of the original speech sound as specified in the tuple
    corresponding to that speech sound in the forced alignment. 

    Parameters
    ----------
    user upload: wav file
    
    Returns
    -------
    mp3 file.
    
"""

    uploaded_wav = AudioSegment.from_file(wavfile_name)
    resampled_wav = uploaded_wav.set_frame_rate(16000)
    resampled_wav.export(wavfile_name, format="wav")

    segments_list = charsiu.align(audio = wavfile_name) # forced alignment of the input wav file
    instrumental_audio = []
    phoneme_list = []
    sound_list = []
    final_audio = AudioSegment.silent(duration=0)

    phoneme_dict = {'UW': 'high back vowel','IY': 'high front vowel','IH': 'high front vowel','EY': 'mid front vowel',
                    'AA': 'low front vowel', 'AY': 'low front vowel', 'AW': 'low back vowel','AO': 'low back vowel', 'AE': 'low front vowel',
                    'UH': 'high back vowel','OY': 'mid back vowel','OW': 'mid back vowel', 'ER': 'mid front vowel','EH': 'mid front vowel','AH': 'mid back vowel',
                    'T': 'voiceless stop','P': 'voiceless stop', 'K': 'voiceless stop','G': 'voiced stop','D': 'voiced stop', 'B': 'voiced stop',
                    'NG': 'nasal','N': 'nasal','M': 'nasal',
                    'SH': 'voiceless fricative', 'DH': 'voiced fricative', 'F': 'voiceless fricative','S': 'voiceless fricative', 'TH': 'voiceless fricative',
                    'V': 'voiced fricative', 'Z': 'voiced fricative', 'ZH': 'voiced fricative','HH': 'voiceless fricative',
                    'CH': 'voiceless affricate', 'JH': 'voiced affricate',
                    'Y': 'approximant','W': 'approximant','R': 'approximant','L': 'approximant',
                    '[SIL]': 'silence'}

    instrument_dict = {
                   'high back vowel' :'/content/drive/MyDrive/Colab Notebooks/highbackvowel.mp3',
                   'high front vowel':'/content/drive/MyDrive/Colab Notebooks/highfrontvowel.mp3',
                   'low back vowel': '/content/drive/MyDrive/Colab Notebooks/lowbackvowel.mp3',
                   'low front vowel': '/content/drive/MyDrive/Colab Notebooks/lowfrontvowel.mp3',
                   'mid back vowel': '/content/drive/MyDrive/Colab Notebooks/midbackvowel.mp3',
                   'mid front vowel': '/content/drive/MyDrive/Colab Notebooks/midfrontvowel.mp3',
                   'voiced stop': '/content/drive/MyDrive/Colab Notebooks/voicedstop.mp3' ,
                   'voiceless stop':'/content/drive/MyDrive/Colab Notebooks/voicelessstop.mp3',
                   'nasal': '/content/drive/MyDrive/Colab Notebooks/nasal.mp3' ,
                   'voiced fricative': '/content/drive/MyDrive/Colab Notebooks/voicedfricative.mp3',
                   'voiceless fricative': '/content/drive/MyDrive/Colab Notebooks/voicedfricative.mp3',
                   'voiced affricate': '/content/drive/MyDrive/Colab Notebooks/voicedaffricate.mp3',
                   'voiceless affricate': '/content/drive/MyDrive/Colab Notebooks/voicelessaffricate.mp3' ,
                   'approximant': '/content/drive/MyDrive/Colab Notebooks/approximant.mp3',
                   'silence': '/content/drive/MyDrive/Colab Notebooks/silence.mp3'}

    for each_segment in segments_list:
      segment_labels = [segment_tuple[2] for segment_tuple in segments_list]

    for each_segment in segment_labels:
        phoneme_list.append(each_segment)

    for each_phoneme in phoneme_list:
        selected_sound = phoneme_dict[each_phoneme]
        sound_list.append(selected_sound)

    for each_sound in sound_list: 
        if each_sound in instrument_dict: 
            selected_instrument = instrument_dict[each_sound]  
            instrumental_audio.append(selected_instrument)

    for mp3_file, segment in zip(instrumental_audio, segments_list):
      audio = AudioSegment.from_mp3(mp3_file)
      duration = segment[1] - segment[0]
      trimmed_audio = audio[50:int(duration*1000)]
      final_audio += trimmed_audio

    return final_audio  #will generate an audio player to listen to the output

#for speech sounds!
from pydub import AudioSegment

def speech_to_speech(wavfile_name):
    """
    Convert a wav file of speech to other speech sounds

     First the function resamples the orignal audio to 16000 Hz. This function uses a forced
    aligner to segment speech sounds. Then it loops though speech sounds to create a list
    with the types of speech sounds present. Then the list of types is looped through and
    specific speech sounds are added to an empty audio segment. The speech sound audios
    are also cliped to the length of the original speech sound as specified in the tuple
    corresponding to that speech sound in the forced alignment.  

    Parameters
    ----------
    user upload: wav file
    
    Returns
    -------
    mp3 file.
  """  

    uploaded_wav = AudioSegment.from_file(wavfile_name)
    resampled_wav = uploaded_wav.set_frame_rate(16000)
    resampled_wav.export(wavfile_name, format="wav")

    segments_list2 = charsiu.align(audio = wavfile_name)
    changed_audio = []
    phoneme_list2 = []
    sound_list2 = []
    final_audio2 = AudioSegment.silent(duration=0)

    phoneme_dict2 = {'UW': 'high vowel','IY': 'high vowel','IH': 'high vowel','EY': 'mid vowel',
                    'AA': 'low vowel', 'AY': 'low vowel', 'AW': 'low vowel','AO': 'low vowel', 'AE': 'low vowel',
                    'UH': 'high vowel','OY': 'mid vowel','OW': 'mid vowel', 'ER': 'mid vowel','EH': 'mid vowel','AH': 'mid vowel',
                    'T': 'stop','P': 'stop', 'K': 'stop','G': 'stop','D': 'stop', 'B': 'stop',
                    'NG': 'nasal','N': 'nasal','M': 'nasal',
                    'SH': 'fricative', 'DH': 'fricative', 'F': 'fricative','S': 'fricative', 'TH': 'fricative',
                    'V': 'fricative', 'Z': 'fricative', 'ZH': 'fricative','HH': 'fricative',
                    'CH': 'affricate', 'JH': 'affricate',
                    'Y': 'approximant','W': 'approximant','R': 'approximant','L': 'approximant',
                    '[SIL]': 'silence'}

    speech_sound_dict = {
                   'high vowel' :'/content/drive/MyDrive/Colab Notebooks/highvowel2.mp3',
                   'low vowel': '/content/drive/MyDrive/Colab Notebooks/lowvowel2.mp3',
                   'mid vowel': '/content/drive/MyDrive/Colab Notebooks/midvowel2.mp3',
                   'stop': '/content/drive/MyDrive/Colab Notebooks/stop2.mp3',
                   'nasal': '/content/drive/MyDrive/Colab Notebooks/nasal2.mp3' ,
                   'fricative': '/content/drive/MyDrive/Colab Notebooks/fricative2.mp3',
                   'affricate': '/content/drive/MyDrive/Colab Notebooks/affricate2.mp3',
                   'approximant': '/content/drive/MyDrive/Colab Notebooks/approximant2.mp3',
                   'silence': '/content/drive/MyDrive/Colab Notebooks/silence.mp3'}

    for each_segment in segments_list2:
      segment_labels2 = [segment_tuple[2] for segment_tuple in segments_list2]

    for each_segment in segment_labels2:
        phoneme_list2.append(each_segment)

    for each_phoneme in phoneme_list2:
        selected_sound = phoneme_dict2[each_phoneme]
        sound_list2.append(selected_sound)

    for each_sound in sound_list2:
        if each_sound in speech_sound_dict:
            selected_speech_sound = speech_sound_dict[each_sound]
            changed_audio.append(selected_speech_sound)

    for mp3_file, segment in zip(changed_audio, segments_list2):
      audio = AudioSegment.from_mp3(mp3_file)
      duration = segment[1] - segment[0]
      trimmed_audio = audio[50:int(duration*1000)]
      final_audio2 += trimmed_audio

    return final_audio2 #will generate an audio player to listen to the output

@article{zhu2022charsiu,
  title={Phone-to-audio alignment without text: A Semi-supervised Approach},
  author={Zhu, Jian and Zhang, Cong and Jurgens, David},
  journal={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2022}
 }
